{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bittextd4ec994d46b44c0997bd7f255dcae23e",
   "display_name": "Python 3.6.9 64-bit ('text')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime\n",
    "from emoji import UNICODE_EMOJI\n",
    "import itertools\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import scattertext as st\n",
    "import nltk\n",
    "import numpy as np\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import json\n",
    "import scattertext as st\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import multidict as multidict\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from stop_words import get_stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"a\",\"aby\",\"aj\",\"ale\",\"anebo\",\"ani\",\"ani≈æ\",\"ano\",\"asi\",\"aspo≈à\",\"atd\",\"atp\",\"az\",\"aƒçkoli\",\"a≈æ\",\"bez\",\"beze\",\"bl√≠zko\",\"bohu≈æel\",\"brzo\",\"bude\",\"budem\",\"budeme\",\"budes\",\"budete\",\"bude≈°\",\"budou\",\"budu\",\"by\",\"byl\",\"byla\",\"byli\",\"bylo\",\"byly\",\"bys\",\"byt\",\"b√Ωt\",\"bƒõhem\",\"chce\",\"chceme\",\"chcete\",\"chce≈°\",\"chci\",\"cht√≠t\",\"chtƒõj√≠\",\"chuti\",\"ci\",\"clanek\",\"clanku\",\"clanky\",\"co\",\"coz\",\"co≈æ\",\"cz\",\"daleko\",\"dalsi\",\"dal≈°√≠\",\"den\",\"deset\",\"design\",\"devaten√°ct\",\"devƒõt\",\"dnes\",\"do\",\"dobr√Ω\",\"docela\",\"dva\",\"dvacet\",\"dvan√°ct\",\"dvƒõ\",\"d√°l\",\"d√°le\",\"dƒõkovat\",\"dƒõkujeme\",\"dƒõkuji\",\"email\",\"ho\",\"hodnƒõ\",\"i\",\"jak\",\"jakmile\",\"jako\",\"jako≈æ\",\"jde\",\"je\",\"jeden\",\"jeden√°ct\",\"jedna\",\"jedno\",\"jednou\",\"jedou\",\"jeho\",\"jeho≈æ\",\"jej\",\"jeji\",\"jejich\",\"jej√≠\",\"jeliko≈æ\",\"jemu\",\"jen\",\"jenom\",\"jen≈æ\",\"jeste\",\"jestli\",\"jestli≈æe\",\"je≈°tƒõ\",\"je≈æ\",\"ji\",\"jich\",\"jimi\",\"jinak\",\"jine\",\"jin√©\",\"jiz\",\"ji≈æ\",\"jsem\",\"jses\",\"jse≈°\",\"jsi\",\"jsme\",\"jsou\",\"jste\",\"j√°\",\"j√≠\",\"j√≠m\",\"j√≠≈æ\",\"j≈°te\",\"k\",\"kam\",\"ka≈æd√Ω\",\"kde\",\"kdo\",\"kdy\",\"kdyz\",\"kdy≈æ\",\"ke\",\"kolik\",\"kromƒõ\",\"ktera\",\"ktere\",\"kteri\",\"kterou\",\"ktery\",\"kter√°\",\"kter√©\",\"kter√Ω\",\"kte≈ôi\",\"kte≈ô√≠\",\"ku\",\"kv≈Øli\",\"ma\",\"maj√≠\",\"mate\",\"me\",\"mezi\",\"mi\",\"mit\",\"mne\",\"mnou\",\"mnƒõ\",\"moc\",\"mohl\",\"mohou\",\"moje\",\"moji\",\"mo≈æn√°\",\"muj\",\"mus√≠\",\"muze\",\"my\",\"m√°\",\"m√°lo\",\"m√°m\",\"m√°me\",\"m√°te\",\"m√°≈°\",\"m√©\",\"m√≠\",\"m√≠t\",\"mƒõ\",\"m≈Øj\",\"m≈Ø≈æe\",\"na\",\"nad\",\"nade\",\"nam\",\"napiste\",\"napi≈°te\",\"naproti\",\"nas\",\"nasi\",\"naƒçe≈æ\",\"na≈°e\",\"na≈°i\",\"ne\",\"nebo\",\"nebyl\",\"nebyla\",\"nebyli\",\"nebyly\",\"nech≈•\",\"nedƒõlaj√≠\",\"nedƒõl√°\",\"nedƒõl√°m\",\"nedƒõl√°me\",\"nedƒõl√°te\",\"nedƒõl√°≈°\",\"neg\",\"nejsi\",\"nejsou\",\"nemaj√≠\",\"nem√°me\",\"nem√°te\",\"nemƒõl\",\"neni\",\"nen√≠\",\"nestaƒç√≠\",\"nevad√≠\",\"nez\",\"ne≈æ\",\"nic\",\"nich\",\"nimi\",\"nove\",\"novy\",\"nov√©\",\"nov√Ω\",\"nula\",\"n√°\",\"n√°m\",\"n√°mi\",\"n√°s\",\"n√°≈°\",\"n√≠\",\"n√≠m\",\"nƒõ\",\"nƒõco\",\"nƒõjak\",\"nƒõkde\",\"nƒõkdo\",\"nƒõmu\",\"nƒõmu≈æ\",\"o\",\"od\",\"ode\",\"on\",\"ona\",\"oni\",\"ono\",\"ony\",\"osm\",\"osmn√°ct\",\"pak\",\"patn√°ct\",\"po\",\"pod\",\"podle\",\"pokud\",\"potom\",\"pouze\",\"pozdƒõ\",\"po≈ô√°d\",\"prave\",\"prav√©\",\"pred\",\"pres\",\"pri\",\"pro\",\"proc\",\"prostƒõ\",\"pros√≠m\",\"proti\",\"proto\",\"protoze\",\"proto≈æe\",\"proƒç\",\"prvni\",\"prvn√≠\",\"pr√°ve\",\"pta\",\"pƒõt\",\"p≈ôed\",\"p≈ôede\",\"p≈ôes\",\"p≈ôese\",\"p≈ôi\",\"p≈ôiƒçem≈æ\",\"re\",\"rovnƒõ\",\"s\",\"se\",\"sedm\",\"sedmn√°ct\",\"si\",\"sice\",\"skoro\",\"sm√≠\",\"smƒõj√≠\",\"snad\",\"spolu\",\"sta\",\"sto\",\"strana\",\"st√©\",\"sve\",\"svych\",\"svym\",\"svymi\",\"sv√©\",\"sv√Ωch\",\"sv√Ωm\",\"sv√Ωmi\",\"sv≈Øj\",\"ta\",\"tady\",\"tak\",\"take\",\"takhle\",\"taky\",\"takze\",\"tak√©\",\"tak≈æe\",\"tam\",\"tamhle\",\"tamhleto\",\"tamto\",\"tato\",\"te\",\"tebe\",\"tebou\",\"ted'\",\"tedy\",\"tema\",\"ten\",\"tento\",\"teto\",\"ti\",\"tim\",\"timto\",\"tipy\",\"tis√≠c\",\"tis√≠ce\",\"to\",\"tobƒõ\",\"tohle\",\"toho\",\"tohoto\",\"tom\",\"tomto\",\"tomu\",\"tomuto\",\"toto\",\"tro≈°ku\",\"tu\",\"tuto\",\"tvoje\",\"tv√°\",\"tv√©\",\"tv≈Øj\",\"ty\",\"tyto\",\"t√©ma\",\"t√©to\",\"t√≠m\",\"t√≠mto\",\"tƒõ\",\"tƒõm\",\"tƒõma\",\"tƒõmu\",\"t≈ôeba\",\"t≈ôi\",\"t≈ôin√°ct\",\"u\",\"urƒçitƒõ\",\"uz\",\"u≈æ\",\"v\",\"vam\",\"vas\",\"vase\",\"va≈°e\",\"va≈°i\",\"ve\",\"vedle\",\"veƒçer\",\"vice\",\"vlastnƒõ\",\"vsak\",\"vy\",\"v√°m\",\"v√°mi\",\"v√°s\",\"v√°≈°\",\"v√≠ce\",\"v≈°ak\",\"v≈°echen\",\"v≈°echno\",\"v≈°ichni\",\"v≈Øbec\",\"v≈ædy\",\"z\",\"za\",\"zat√≠mco\",\"zaƒç\",\"zda\",\"zde\",\"ze\",\"zpet\",\"zpravy\",\"zpr√°vy\",\"zpƒõt\",\"ƒçau\",\"ƒçi\", \"ƒçtrn√°ct\",\"ƒçty≈ôi\",\"≈°est\",\"≈°estn√°ct\",\"≈æe\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sw in get_stop_words('cz'):\n",
    "    if not (sw in stopwords):\n",
    "        stopwords.append(sw)"
   ]
  },
  {
   "source": [
    "## GET DATA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath = None, text = None):\n",
    "    if filepath:       \n",
    "        with open(filepath, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "    elif text:\n",
    "        lines = [text]\n",
    "    else:\n",
    "        raise Exception(\"Neither text or filepath was entered\")\n",
    "\n",
    "    soup = BeautifulSoup(lines[0], \"html.parser\")\n",
    "    messages_to = soup.title.text\n",
    "    names = [n.text for n in soup.findAll(\"div\", {\"class\": \"_3-96 _2pio _2lek _2lel\"})]\n",
    "    messages = [m.text for m in soup.findAll(\"div\", {\"class\": \"_3-96 _2let\"})]\n",
    "    times = [t.text for t in soup.findAll(\"div\", {\"class\": \"_3-94 _2lem\"})]\n",
    "    all_divs = [a.text for a in soup.findAll(\"div\", {\"class\": \"pam _3-95 _2pi0 _2lej uiBoxWhite noborder\"})]\n",
    "\n",
    "    names.reverse()\n",
    "    times.reverse()\n",
    "    messages.reverse()\n",
    "\n",
    "    return list(zip(names, times, messages)), names, times, messages\n",
    "\n",
    "def str_to_time(s_time):\n",
    "    # date_time_str = '2018-06-29 08:15:27.243860'\n",
    "    return datetime.datetime.strptime(s_time, '%d. %m. %Y %H:%M')\n",
    "\n",
    "def get_emoji_reaction(m, w):\n",
    "    if m.endswith(w):\n",
    "        emoji_position = len(m) - len(w) - 2 \n",
    "        emoji_ = m[emoji_position]\n",
    "        if is_emoji(emoji_):\n",
    "            return emoji_\n",
    "    return None\n",
    "\n",
    "def delete_reaction_end(m, writers):\n",
    "    for w in writers:\n",
    "        emoji = get_emoji_reaction(m,w)\n",
    "        if emoji:\n",
    "            return m[:len(m)-len(w)-2]\n",
    "    return m\n",
    "\n",
    "def replace_hidden_emoji(m):\n",
    "    new_m = []\n",
    "    laugh_pattern = '^.*(:[dD])+.*$'\n",
    "    smile_pattern = \"^.*(:\\))+.*$\"\n",
    "    smile = re.compile(smile_pattern)\n",
    "    laugh = re.compile(laugh_pattern)\n",
    "    for w in m.split():\n",
    "        laugh_match = laugh.search(w)\n",
    "        smile_match = smile.search(w)\n",
    "        new_word = w\n",
    "\n",
    "        if laugh_match:\n",
    "            splitted = w.split(\":\")\n",
    "            if splitted[-1].lower() == \"d\":\n",
    "                    new_word = new_word[:-2] + \"üòÄ\"        \n",
    "        elif smile_match:\n",
    "            splitted = w.split(\":\")\n",
    "            if splitted[-1].lower() == \")\":\n",
    "                    new_word = new_word[:-2] + \"üòä\"\n",
    "\n",
    "        new_m.append(new_word)\n",
    "    return \" \".join(new_m)\n",
    "\n",
    "def is_emoji(s):\n",
    "    count = 0\n",
    "    for emoji in UNICODE_EMOJI:\n",
    "        count += s.count(emoji)\n",
    "        if count > 1:\n",
    "            return False\n",
    "    return bool(count)\n",
    "\n",
    "def is_emoji_in_message(message):\n",
    "    for c in message:\n",
    "        if is_emoji(c):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def extract_emojis(s):\n",
    "    emojis = []\n",
    "    for word in s:\n",
    "        for c in word:\n",
    "            if c in [\"‚ôÇ\",\"‚ôÄ\"]:\n",
    "                continue\n",
    "            if is_emoji(c):\n",
    "                emojis.append(c)\n",
    "    return emojis\n",
    "\n",
    "def get_writers_messages(messages, names):\n",
    "    writers_messages = {}\n",
    "    for i in range(len(messages)):\n",
    "        writers_messages[names[i]] = writers_messages.get(names[i], []) + [messages[i]]\n",
    "    return writers_messages\n",
    "\n",
    "\n",
    "def get_writers_words(messages, names):\n",
    "    writers_words = {}\n",
    "    for i in range(len(messages)):\n",
    "        message = messages[i]\n",
    "        writer = names[i]\n",
    "        words = messages[i].split()\n",
    "        for word in words:\n",
    "            writers_words[writer] = writers_words.get(writer, []) + [word.lower()]\n",
    "    return writers_words\n",
    "\n",
    "def is_reacted_message(m, writers):\n",
    "    for w in writers:\n",
    "        if m.endswith(w):\n",
    "            emoji_position = len(m) - len(w) - 2 \n",
    "            emoji_ = m[emoji_position]\n",
    "            if is_emoji(emoji_):\n",
    "                return emoji_\n",
    "    return None\n"
   ]
  },
  {
   "source": [
    "## Get data from file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(filepath = None, text = None, hide_authors = False):\n",
    "    data, n, t, m = read_file(filepath, text)\n",
    "    names, times, messages = [], [], []\n",
    "    for i in range(len(data)):\n",
    "        if t and n:\n",
    "            names.append(n[i])\n",
    "            times.append(t[i])\n",
    "            messages.append(replace_hidden_emoji(m[i]))\n",
    "\n",
    "    writers = list(set(names))\n",
    "    messages_with_reactions = messages\n",
    "    messages_without_reactions = [replace_hidden_emoji(delete_reaction_end(m, writers)) for m in messages]\n",
    "\n",
    "    times_string = times\n",
    "    times_datetime = [str_to_time(t) for t in times]\n",
    "\n",
    "    return names, times, writers, messages, messages_without_reactions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath2 = \"/home/vena/Documents/ai/fb_mess/data1/dominikhanke_thycdq67qw/message_1.html\"\n",
    "filepath1 = '/home/vena/Documents/ai/fb_mess/data1/annamickova_pwbfftazwa/message_1.html'\n",
    "filepath3 = \"/home/vena/Documents/ai/fb_mess/data1/tripvol3_kjltaezdrw/message_1.html\"\n",
    "filepath = '/home/vena/Documents/ai/fb_mess/data1/annamickova_pwbfftazwa/message_1.html'\n",
    "\n",
    "names, times, writers, messages, messages_without_reactions = get_data(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics(messages, names, writers):\n",
    "    writers_words = get_writers_words(messages_without_reactions, names)\n",
    "    writers_messages = get_writers_messages(messages_without_reactions, names)\n",
    "\n",
    "    print(\"---Number of messages---\")\n",
    "    for writer in writers:\n",
    "        print(f\"{writer}: {len(writers_messages[writer])}\")\n",
    "\n",
    "    print(\"\\n---Number of words---\")\n",
    "    for writer in writers:\n",
    "        print(f\"{writer}: {sum([len(mes.split()) for mes in writers_messages[writer]])}\")\n",
    "\n",
    "    print(\"\\n---Number of characters---\")\n",
    "    for writer in writers:\n",
    "        print(f\"{writer}: {sum([len(mes) for mes in writers_messages[writer]])}\")\n",
    "\n",
    "    print(\"\\n---Average message length (words)---\")\n",
    "    for writer in writers:\n",
    "        print(f\"{writer}: {round(sum([len(mes.split()) for mes in writers_messages[writer]]) / len(writers_messages[writer]), 2)}\")\n",
    "\n",
    "    print(\"\\n---Longest message (words)---\")\n",
    "    for writer in writers:\n",
    "        print(f\"{writer}: {max([len(mes.split()) for mes in writers_messages[writer]])}\")\n",
    "\n",
    "    print(\"\\n---Number of questions---\")\n",
    "    for writer in writers:\n",
    "        print(f\"{writer}: {len([mes for mes in writers_messages[writer] if ('?' in mes)])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_general_pies(writers, names, messages, nonames=False):\n",
    "    writers_messages = get_writers_messages(messages, names)\n",
    "    messages_count = [len(writers_messages[writers[i]]) for i in range(len(writers))]\n",
    "    words_count = [sum([len(mes.split()) for mes in writers_messages[writers[i]]])for i in range(len(writers))]\n",
    "    chars_count = [sum([len(mes) for mes in writers_messages[writers[i]]]) for i in range(len(writers))]\n",
    "    questions_count = [len([mes for mes in writers_messages[writer] if ('?' in mes)]) for writer in writers]\n",
    "    emojis_count = [len([mes for mes in writers_messages[writer] if (is_emoji_in_message(mes))]) for writer in writers]\n",
    "    lenghty_count = [len([mes for mes in writers_messages[writer] if (len(mes.split()) > 10)]) for writer in writers]\n",
    "\n",
    "\n",
    "\n",
    "    if nonames:\n",
    "        writers = [\"Author_\" + f\"{i}\" for i in range(len(writers))]\n",
    "\n",
    "    # colors = [\"mediumturquoise\", \"darkorange\"]\n",
    "    fig = go.FigureWidget(make_subplots(rows = 2, cols=3, specs=[[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}], [{'type':'domain'}, {'type':'domain'}, {'type':'domain'}]], subplot_titles=['Poƒçet zpr√°v', 'Poƒçet slov', 'Poƒçet znak≈Ø', \"Zpr√°vy s ot√°zkou\", \"Zpr√°vy s emoji\", \"Zpr√°vy > 10 slov\"]))\n",
    "    \n",
    "    trace0 = go.Pie(labels=writers, values=messages_count, name=\"Celkov√Ω poƒçet zpr√°v\")\n",
    "    trace1 = go.Pie(labels=writers, values=words_count, name='Celkov√Ω poƒçet slov')\n",
    "    trace2 = go.Pie(labels=writers, values=chars_count, name=\"Celkov√Ω poƒçet znak≈Ø\")\n",
    "    trace3 = go.Pie(labels=writers, values=questions_count, name=\"Celkov√Ω poƒçet ot√°zek\")\n",
    "    trace4 = go.Pie(labels=writers, values=emojis_count, name=\"Celkov√Ω poƒçet emoji\")\n",
    "    trace5 = go.Pie(labels=writers, values=lenghty_count, name=\"Celkov√Ω poƒçet dlouh√Ωch zpr√°v\")\n",
    "\n",
    "    fig.add_trace(trace0, 1, 1)\n",
    "    fig.add_trace(trace1, 1, 2)\n",
    "    fig.add_trace(trace2, 1, 3)\n",
    "    fig.add_trace(trace3, 2, 1)\n",
    "    fig.add_trace(trace4, 2, 2)\n",
    "    fig.add_trace(trace5, 2, 3)\n",
    "\n",
    "\n",
    "    fig.update_traces(hoverinfo=\"label+percent\", textinfo=\"value\", textfont_size=12, marker=dict(line=dict(color=\"black\", width=2)))\n",
    "    fig.update_layout(title_text = \"Statistiky o poƒçtu zpr√°v, slov a znak≈Ø\", width=1200, height=700)\n",
    "    \n",
    "    # fig.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# display_general_pies(writers, names, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_messages_length_whisker_plots(messages, writers, names, nonames=False):\n",
    "    writers_messages = get_writers_messages(messages, names)\n",
    "    m_lengths = [[len(m) for m in writers_messages[writer]] for writer in writers]\n",
    "    m_words = [[len(m.split()) for m in writers_messages[writer]] for writer in writers]\n",
    "\n",
    "    fig = go.FigureWidget()\n",
    "\n",
    "    if nonames:\n",
    "        writers = [\"Author_\" + f\"{i}\" for i in range(len(writers))]\n",
    "    for i, writer in enumerate(writers):\n",
    "        fig.add_trace(go.Violin(y=m_words[i],\n",
    "                                name=writer,\n",
    "                                box_visible=True,\n",
    "                                meanline_visible=True,\n",
    "                                points=\"all\"))\n",
    "\n",
    "    fig.update_layout(title=\"Rozlo≈æen√≠ poƒçtu slov jednotliv√Ωch pisatel≈Ø\", yaxis_title = \"Poet slov\", width = 1200, height=600)\n",
    "\n",
    "    # fig.show()\n",
    "    return fig\n",
    "\n",
    "# display_messages_length_whisker_plots(messages_without_reactions, writers, names, True)    "
   ]
  },
  {
   "source": [
    "## TIME THINGS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timedeltas(names, times):\n",
    "    time_series = {}\n",
    "    last_writer, last_time = None, None\n",
    "\n",
    "    for i in range(len(names)):\n",
    "        respondent = names[i]\n",
    "        m_time = str_to_time(times[i])\n",
    "\n",
    "        if not last_writer:\n",
    "            last_writer = respondent \n",
    "            last_time = m_time\n",
    "            time_series[respondent] = time_series.get(respondent, []) + [0]\n",
    "        else:\n",
    "            time_delta = (m_time - last_time).total_seconds() / 60.0\n",
    "            if respondent != last_writer:\n",
    "                time_series[respondent] = time_series.get(respondent, []) + [time_delta]\n",
    "                last_writer = respondent\n",
    "                last_time = m_time\n",
    "            else: # Not from the very last message \n",
    "                time_series[respondent] = time_series.get(respondent, []) + [-1]\n",
    "                last_time = m_time\n",
    "    return time_series\n",
    "\n",
    "def get_avg_time_to_answear(writers, names, times):\n",
    "    time_series = get_timedeltas(names, times)\n",
    "\n",
    "    print(\"---Min time to answear---\")\n",
    "    min_times = {}\n",
    "    for writer in writers:\n",
    "        min_time = round(min(time_series[writer]) / 60, 1)\n",
    "        min_times[writer] = min_time\n",
    "        print(f\"{writer}: {min_time} minutes\")\n",
    "\n",
    "    print(\"\\n---Max time to answear---\")\n",
    "    max_times = {}\n",
    "    for writer in writers:\n",
    "        max_time = round(max(time_series[writer]) / 60, 1)\n",
    "        max_times[writer] = max_time\n",
    "        print(f\"{writer}: {max_time} minutes\")\n",
    "    \n",
    "    print(\"\\n---Average time to answear---\")\n",
    "    avg_times = {}\n",
    "    for writer in writers:\n",
    "        avg_time = round(sum(time_series[writer]) / (len(time_series[writer]) * 60), 1)\n",
    "        avg_times[writer] = avg_time\n",
    "        print(f\"{writer}: {avg_time} minutes\")\n",
    "\n",
    "    return avg_times\n",
    "\n",
    "def get_immediate_responses_count(writers, names, times, immediate = 5):\n",
    "    time_series = get_timedeltas(names, times)\n",
    "    \n",
    "    # print(\"\\n---Immediate responses (less than 5 minute)---\")\n",
    "    zero_times = {}\n",
    "    for writer in writers:\n",
    "        zero_count = len([t for t in time_series[writer] if (t < immediate and t >= 0)])\n",
    "        zero_times[writer] = zero_count\n",
    "        # print(f\"{writer}: {zero_count}\")\n",
    "\n",
    "    return zero_times\n",
    "\n",
    "def get_long_responses_count(writers, names, times, immediate = 60):\n",
    "    time_series = get_timedeltas(names, times)\n",
    "    \n",
    "    # print(f\"\\n---Immediate responses (less than {immediate} minutes)---\")\n",
    "    zero_times = {}\n",
    "    for writer in writers:\n",
    "        zero_count = len([t for t in time_series[writer] if (abs(t) > immediate)])\n",
    "        zero_times[writer] = zero_count\n",
    "        # print(f\"{writer}: {zero_count}\")\n",
    "\n",
    "    return zero_times\n",
    "\n",
    "def get_longest_message_streak(names, messages):\n",
    "    last_name = None\n",
    "    longest_streak = {}\n",
    "    longest_streak_messages = {}\n",
    "    starts_from = {}\n",
    "    message_streak = []\n",
    "    current_streak = 1\n",
    "    for index, name in enumerate(names):\n",
    "        if not last_name:\n",
    "            last_name = name\n",
    "        else:\n",
    "            if last_name == name:\n",
    "                current_streak += 1\n",
    "                message_streak.append(messages[index])\n",
    "            else:\n",
    "                if longest_streak.get(last_name, 1) < current_streak:\n",
    "                    longest_streak[last_name] = current_streak\n",
    "                    longest_streak_messages[last_name] = message_streak\n",
    "\n",
    "                current_streak = 1\n",
    "                message_streak = []\n",
    "            last_name = name\n",
    "    \n",
    "    return longest_streak_messages()\n",
    "\n",
    "def get_conversations_starters(writers,names, times, threshold = 24 * 60):\n",
    "    conv_started = {}\n",
    "    for writer in writers:\n",
    "        conv_started[writer] = 0\n",
    "    last_writer, last_time = None, None\n",
    "\n",
    "    for i in range(len(names)):\n",
    "        respondent = names[i]\n",
    "        m_time = str_to_time(times[i])\n",
    "\n",
    "        if i == 0:\n",
    "            conv_started[respondent] = conv_started.get(respondent, 0) + 1\n",
    "        else:\n",
    "            time_delta = (m_time - last_time).total_seconds() / 60.0\n",
    "            if time_delta > threshold:\n",
    "                conv_started[respondent] = conv_started.get(respondent, 0) + 1\n",
    "\n",
    "        last_time = m_time\n",
    "\n",
    "    return conv_started\n",
    "\n",
    "def get_writers_timedeltas(i, writers):\n",
    "    timedeltas = get_timedeltas(names, times)\n",
    "    return [t for t in timedeltas[writers[i]] if t != -1]\n",
    "\n",
    "def get_weekdays(writers, names, times):\n",
    "    writers_weekdays = {}\n",
    "    day_name= ['Pondƒõl√≠', '√öter√Ω', 'St≈ôeda', 'ƒåtvrtek', 'P√°tek', 'Sobota','Nedƒõle']\n",
    "    for i in range(len(names)):\n",
    "        t = times[i]\n",
    "        writer = names[i]\n",
    "        writers_weekdays[writer] = writers_weekdays.get(writer, []) + [day_name[str_to_time(t).weekday()]]\n",
    "\n",
    "    return writers_weekdays\n",
    "\n",
    "def writer_messages_in_a_day(dates, writer, times, messages):\n",
    "    dates_messages = [[] for i in dates]\n",
    "    for index, time in enumerate(times):\n",
    "        if names[index] != writer:\n",
    "            continue\n",
    "        date = str_to_time(time).date()\n",
    "        message = messages[index]\n",
    "        for i, d in enumerate(dates):\n",
    "            if d == date:\n",
    "                dates_messages[i] = dates_messages[i] + [message]\n",
    "    return dates_messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_conversations_pies(writers, names, times, messages, nonames=False):\n",
    "    colors = [\"mediumturquoise\", \"darkorange\"]\n",
    "    immediate_responses = [i for i in get_immediate_responses_count(writers, names, times, 5).values()]\n",
    "    long_responses = [i for i in get_immediate_responses_count(writers, names, times, 2 * 60).values()]\n",
    "    conv_starters = [i for i in get_conversations_starters(writers, names, times, 36 * 60).values()]\n",
    "\n",
    "    if nonames:\n",
    "        writers = [\"Author_\" + f\"{i}\" for i in range(len(writers))]\n",
    "    \n",
    "    fig = go.FigureWidget(make_subplots(rows = 1, cols=3, specs=[[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}]], subplot_titles=['Do 5 minut', 'Do 2 hodin', \"Starty konverzac√≠\"]))\n",
    "    trace0 = go.Pie(labels=writers, values=immediate_responses, name=\"m√©nƒõ ne≈æ 5 minut\")\n",
    "    trace1 = go.Pie(labels=writers, values=long_responses, name='m√©nƒõ jak 2 hodiny')\n",
    "    trace2 = go.Pie(labels=writers, values=conv_starters, name='v√≠ce jak 24 hodin')\n",
    "\n",
    "    fig.add_trace(trace0, 1, 1)\n",
    "    fig.add_trace(trace1, 1, 2)\n",
    "    fig.add_trace(trace2, 1, 3)\n",
    "\n",
    "    fig.update_traces(hoverinfo=\"label+percent\", textinfo=\"value\", textfont_size=15, marker=dict(line=dict(color=\"black\", width=2)))\n",
    "    fig.update_layout(title_text = \"Rychlosti odpovƒõd√≠\", width=1200)\n",
    "    # fig.show()\n",
    "    return fig\n",
    "\n",
    "# display_conversations_pies(writers, names, times, messages_without_reactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_answer_time_histogram(writers, names, times, nonames=False):\n",
    "        timedeltas = get_timedeltas(names, times)\n",
    "        hist_data = [get_writers_timedeltas(i, writers) for i in range(len(writers))]\n",
    "        group_labels = writers\n",
    "        colors = [\"#393E46\", \"#F66095\"]\n",
    "\n",
    "        if nonames:\n",
    "                writers = [\"Author_\" + f\"{i}\" for i in range(len(writers))]\n",
    "        fig = go.FigureWidget()\n",
    "        for i in range(len(writers)):\n",
    "                fig.add_trace(go.Histogram(histfunc=\"count\", x = hist_data[i], name=writers[i], xbins=dict(\n",
    "                        start=0.0,\n",
    "                        end=30.0,\n",
    "                        size=1)))\n",
    "\n",
    "        fig.update_layout(barmode=\"overlay\",title=\"Jak rychle odpov√≠d√°m? (0 a≈æ 30 min)\", xaxis_title='Doba odpovƒõdi (min)', yaxis_title='Poƒçet odpovƒõd√≠', width=1200)\n",
    "        fig.update_traces(opacity=0.5)\n",
    "\n",
    "        # fig.update_traces(hoverinfo=\"label\")\n",
    "        # fig.show()\n",
    "        return fig\n",
    "\n",
    "# display_answer_time_histogram(writers, names, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_messages_in_weekdays_histogram(writers, names, times, nonames=False):\n",
    "    day_name= ['Pondƒõl√≠', '√öter√Ω', 'St≈ôeda', 'ƒåtvrtek', 'P√°tek', 'Sobota','Nedƒõle']\n",
    "    writers_weekdays = get_weekdays(writers, names, times)\n",
    "    hist_data = [writers_weekdays[writers[i]] for i in range(len(writers))]\n",
    "    \n",
    "    for i, w in enumerate(writers):\n",
    "        days = hist_data[i]\n",
    "        new_days = []\n",
    "        weekdays_counter = [0] * len(day_name)\n",
    "        for d in days:\n",
    "            for k, weekday in enumerate(day_name):\n",
    "                if d == weekday:\n",
    "                    weekdays_counter[k] += 1\n",
    "\n",
    "        for j, counter in enumerate(weekdays_counter):\n",
    "            new_days = new_days + [day_name[j] for _ in range(counter)]\n",
    "        hist_data[i] = new_days\n",
    "\n",
    "    if nonames:\n",
    "        writers = [\"Author_\" + f\"{i}\" for i in range(len(writers))]\n",
    "\n",
    "    fig = go.FigureWidget()\n",
    "    for i in range(len(writers)):\n",
    "        fig.add_trace(go.Histogram(histfunc=\"count\", x = hist_data[i], name=writers[i]))\n",
    "\n",
    "    fig.update_layout(barmode=\"stack\",title=\"Agregovan√Ω poƒçet zpr√°v ve dnech\", xaxis_title='Den v t√Ωdnu', yaxis_title='Poƒçet zpr√°v', width=1000)\n",
    "    # fig.update_traces(hoverinfo=\"label\")\n",
    "    # fig.show()\n",
    "    return fig\n",
    "\n",
    "# display_messages_in_weekdays_histogram(writers, names, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_half_hours_histogram(writers, names, times, nonames=False):\n",
    "    hours = [str_to_time(t).hour + (str_to_time(t).minute / 60) for t in times]\n",
    "    writers_hours = [[] for writer in writers]\n",
    "\n",
    "    for i, h in enumerate(hours):\n",
    "        writer = names[i]\n",
    "        for j, ww in enumerate(writers):\n",
    "            if ww == writer:\n",
    "                writers_hours[j].append(h)\n",
    "\n",
    "    if nonames:\n",
    "        writers = [\"Author_\" + f\"{i}\" for i in range(len(writers))]\n",
    "\n",
    "    fig = go.FigureWidget()\n",
    "    for i in range(len(writers)):\n",
    "        fig.add_trace(go.Histogram(x=writers_hours[i], xbins=dict(start=0, end=24, size=1), name=writers[i]))\n",
    "\n",
    "    fig.update_layout(barmode=\"stack\", title_text = \"Agregovan√Ω poƒçet zpr√°v v p≈Ølhodinov√Ωch intervalech\", xaxis_title = \"Hodiny\", yaxis_title = \"Poƒçet zpr√°v\", width = 1000)\n",
    "\n",
    "    # fig.update_traces(opacity=0.75)\n",
    "    # fig.show()\n",
    "    return fig\n",
    "\n",
    "# display_half_hours_histogram(writers, names, times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "FigureWidget({\n    'data': [{'marker': {'line': {'width': 2}, 'size': 10},\n              'mode': 'markers+line‚Ä¶",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bdb7bacb1dd8444eb568e695d3c853a6"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "def display_messages_in_time_scatter(writers, names, times, messages, nonames=False):\n",
    "    dates = sorted(list(set([str_to_time(t).date() for t in times])))\n",
    "    day_messages = {}\n",
    "    len_day_messages = {}\n",
    "    for writer in writers:\n",
    "        day_messages[writer] = writer_messages_in_a_day(dates, writer, times, messages)\n",
    "        len_day_messages[writer] = [len(ms) for ms in day_messages[writer]]\n",
    "\n",
    "    if nonames:\n",
    "        writers = [\"Author_\" + f\"{i}\" for i in range(len(writers))]\n",
    "        new_len_day = {}\n",
    "        for i, w in enumerate(list(len_day_messages.keys())):\n",
    "            new_len_day[writers[i]] = len_day_messages[w]\n",
    "        len_day_messages = new_len_day\n",
    "\n",
    "    dates = [\"{}-{}-{}\".format(o.year, o.month, o.day) for o in dates]\n",
    "            \n",
    "    \n",
    "    fig = go.FigureWidget()\n",
    "    for i in range(len(writers)):\n",
    "        fig.add_trace(go.Scatter(x=dates, y=len_day_messages[writers[i]], name=writers[i]))\n",
    "    # fig.add_trace(go.Scatter(x=dates, y=len_day_messages[writers[1]], name=writers[1]))\n",
    "\n",
    "    fig.update_traces(mode=\"markers+lines\", marker_size=10, marker_line_width=2)\n",
    "    fig.update_layout(title=\"Poƒçet zpr√°v v ƒçase\", yaxis_zeroline=False, xaxis_zeroline=False, width=1200)\n",
    "\n",
    "    # fig.show()\n",
    "    return fig\n",
    "\n",
    "# display_messages_in_time_scatter(writers, names, times, messages)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## WORD THINGS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chars(words_list):\n",
    "    chars = {}\n",
    "    for word in words_list:\n",
    "        for c in word:\n",
    "            chars[c] = chars.get(c, 0) + 1\n",
    "    return chars\n",
    "    \n",
    "def get_bow(words_list):\n",
    "    bag_of_words = {}\n",
    "    for line in words_list:\n",
    "        if line in bag_of_words:\n",
    "            bag_of_words[line] += 1\n",
    "        else:\n",
    "            bag_of_words[line] = 1\n",
    "    \n",
    "    return dict(sorted(bag_of_words.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "def get_n_grams(words_list, n=2):\n",
    "    buffer = []\n",
    "    n_grams = {}\n",
    "    for i, word in enumerate(words_list):\n",
    "        buffer.append(word)\n",
    "\n",
    "        if len(buffer) == n:\n",
    "            n_gram_tuple = tuple(buffer)\n",
    "            buffer = [buffer[-1]]\n",
    "\n",
    "            if n_gram_tuple in n_grams:\n",
    "                n_grams[n_gram_tuple] += 1\n",
    "            else:\n",
    "                n_grams[n_gram_tuple] = 1\n",
    "    return n_grams  \n",
    "\n",
    "def get_messages_containing(messages, word):\n",
    "    ms = []\n",
    "    for m in messages:\n",
    "        for w in m.split():\n",
    "            if w.lower() == word.lower():\n",
    "                ms.append(m)\n",
    "    return ms\n",
    "\n",
    "def get_most_n_grams(writers, messages, names, stopwords = [], count = 10, n=2):\n",
    "    writers_words = get_writers_words(messages, names)\n",
    "    most_used_ngrams = {}\n",
    "    for writer in writers:\n",
    "        words = writers_words[writer]\n",
    "        if stopwords:\n",
    "            words = [word for word in writers_words[writer] if word not in stopwords]\n",
    "        \n",
    "\n",
    "        my_words = get_n_grams(words, n=n)\n",
    "        sorted_words = dict(sorted(my_words.items(), key=lambda item: item[1], reverse=True))\n",
    "        sliced_words = dict(itertools.islice(sorted_words.items(), count))\n",
    "\n",
    "        most_used_ngrams[writer] = sliced_words\n",
    "\n",
    "    return most_used_ngrams\n",
    "\n",
    "def most_used_words(writers, messages, names, stopwords = [], count = 10):\n",
    "    writers_words = get_writers_words(messages, names)\n",
    "    most_used_words = {}\n",
    "    for writer in writers:\n",
    "        words = writers_words[writer]\n",
    "        if stopwords:\n",
    "            words = [word for word in writers_words[writer] if word not in stopwords]\n",
    "        \n",
    "\n",
    "        my_words = get_bow(words)\n",
    "        sorted_words = dict(sorted(my_words.items(), key=lambda item: item[1], reverse=True))\n",
    "        sliced_words = dict(itertools.islice(sorted_words.items(), count))\n",
    "\n",
    "        most_used_words[writer] = sliced_words\n",
    "\n",
    "    return most_used_words\n",
    "\n",
    "def is_emoji_word(word):\n",
    "    for c in word:\n",
    "        if is_emoji(c):\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "def get_freq_dict(writers, messages, names, stopwords = []):\n",
    "    writers_words = get_writers_words(messages, names)\n",
    "    most_used_words = {}\n",
    "    words = []\n",
    "    for writer in writers:\n",
    "        new_words = writers_words[writer]\n",
    "        new_words = [word for word in new_words if not is_emoji_word(word)]\n",
    "\n",
    "        if stopwords:\n",
    "            words = [word for word in new_words if word not in stopwords]\n",
    "        words += new_words\n",
    "        \n",
    "\n",
    "    my_words = get_bow(words)\n",
    "    sorted_words = dict(sorted(my_words.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    return sorted_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_most_n_grams(writers,messages_without_reactions, names, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_used_words(writers, messages_without_reactions, names, stopwords)"
   ]
  },
  {
   "source": [
    "### ScatterText"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scatter_text(writers, names, messages, nonames=False):\n",
    "    my_df = pd.DataFrame({\"author\": names, \"message\": messages})\n",
    "    nlp = st.tweet_tokenizier_factory(nltk.tokenize.TweetTokenizer())\n",
    "    my_df['parse'] = my_df['message'].apply(nlp)\n",
    "\n",
    "    # df = st.SampleCorpora.ConventionData2012.get_data().assign(\n",
    "    #     parse=lambda df: df.text.apply(st.whitespace_nlp_with_sentences)\n",
    "    # )\n",
    "\n",
    "    corpus = st.CorpusFromParsedDocuments(\n",
    "        my_df, category_col='author', parsed_col='parse'\n",
    "    ).build().get_unigram_corpus().compact(st.AssociationCompactor(2000))\n",
    "\n",
    "    if nonames:\n",
    "        html = st.produce_scattertext_explorer(\n",
    "        corpus,\n",
    "        category=writers[0], category_name=\"Author_0\", not_category_name=\"Author_1\",\n",
    "        minimum_term_frequency=0, pmi_threshold_coefficient=0,\n",
    "        width_in_pixels=1000,# metadata=corpus.get_df()['speaker'],\n",
    "        transform=st.Scalers.dense_rank\n",
    "    )\n",
    "    else:\n",
    "        html = st.produce_scattertext_explorer(\n",
    "            corpus,\n",
    "            category=writers[0], category_name=writers[0], not_category_name=writers[1],\n",
    "            minimum_term_frequency=0, pmi_threshold_coefficient=0,\n",
    "            width_in_pixels=1000,# metadata=corpus.get_df()['speaker'],\n",
    "            transform=st.Scalers.dense_rank\n",
    "        )\n",
    "\n",
    "    with open('./demo_compact.html', 'w') as f:\n",
    "        f.write(html)\n",
    "    f.close()"
   ]
  },
  {
   "source": [
    "### WordCloud"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFrequencyDictForText(sentence):\n",
    "    fullTermsDict = multidict.MultiDict()\n",
    "    tmpDict = {}\n",
    "\n",
    "    # making dict for counting frequencies\n",
    "    for text in sentence.split(\" \"):\n",
    "        val = tmpDict.get(text, 0)\n",
    "        tmpDict[text.lower()] = val + 1\n",
    "    for key in tmpDict:\n",
    "        fullTermsDict.add(key, tmpDict[key])\n",
    "    return fullTermsDict\n",
    "\n",
    "\n",
    "def makeImage(text, image_file):\n",
    "    alice_mask = np.array(Image.open(image_file))\n",
    "\n",
    "    wc = WordCloud(background_color=\"white\", max_words=1000, mask=alice_mask)\n",
    "    # generate word cloud\n",
    "    wc.generate_from_frequencies(text)\n",
    "    wc.to_file(\"used_words.png\")\n",
    "    # show\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_word_cloud(writers, messages, names, image_file):\n",
    "    # get data directory (using getcwd() is needed to support running example in generated IPython notebook)\n",
    "    d = path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "\n",
    "    makeImage(get_freq_dict(writers, messages, names, stopwords), image_file)\n",
    "\n",
    "# create_word_cloud(writers, messages_without_reactions, names, \"fb_logo.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grey_color_func(word, font_size, position, orientation, random_state=None,\n",
    "                    **kwargs):\n",
    "    return \"hsl(0, 0%%, %d%%)\" % random.randint(60, 100)\n",
    "\n",
    "def custom_word_cloud(messages):\n",
    "    # get data directory (using getcwd() is needed to support running example in generated IPython notebook)\n",
    "    d = path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "\n",
    "    # read the mask image taken from\n",
    "    # http://www.stencilry.org/stencils/movies/star%20wars/storm-trooper.gif\n",
    "    mask = np.array(Image.open(path.join(d, \"fb_logo.png\")))\n",
    "\n",
    "    text = []\n",
    "    for m in messages:\n",
    "        text.append(m)\n",
    "    text = \" \".join(text)\n",
    "    # print(type(text))\n",
    "\n",
    "    # adding movie script specific stopwords\n",
    "    stopwords = set(STOPWORDS)\n",
    "    stopwords.add(\"int\")\n",
    "    stopwords.add(\"ext\")\n",
    "\n",
    "    wc = WordCloud(max_words=1000, mask=mask, stopwords=stopwords, margin=10,\n",
    "                random_state=1).generate(text)\n",
    "    # store default colored image\n",
    "    default_colors = wc.to_array()\n",
    "    plt.title(\"Custom colors\")\n",
    "    plt.imshow(wc.recolor(color_func=grey_color_func, random_state=3),\n",
    "            interpolation=\"bilinear\")\n",
    "    wc.to_file(\"a_new_hope.png\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.figure()\n",
    "    plt.title(\"Default colors\")\n",
    "    plt.imshow(default_colors, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "source": [
    "## EMOJIS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emoji_reactions(messages):\n",
    "    emoji_reactions = {}\n",
    "    for message in messages:\n",
    "        for w in writers:\n",
    "            emoji = get_emoji_reaction(message, w)\n",
    "            if emoji:\n",
    "                emoji_reactions[w] = emoji_reactions.get(w, []) + [emoji]\n",
    "    return emoji_reactions\n",
    "\n",
    "\n",
    "def get_messages_which_was_reacted_to(writers, names, messages, w):\n",
    "    reacted_messages = []\n",
    "    reactions = []\n",
    "    reactors = []\n",
    "    for i, m in enumerate(messages):\n",
    "        if is_reacted_message(m, writers) and names[i] == w:\n",
    "            reacted_messages.append(replace_hidden_emoji(delete_reaction_end(m, writers)))\n",
    "            for writer in writers:\n",
    "                emoji = get_emoji_reaction(m, writer)\n",
    "                if emoji:\n",
    "                    reactions.append(emoji)\n",
    "                    reactors.append(writer)\n",
    "                    break\n",
    "    return list(zip(reacted_messages, reactions, reactors))     \n",
    "\n",
    "def get_writers_emojis(writers, messages):\n",
    "    writers_emojis = {}\n",
    "    for i, m in enumerate(messages):\n",
    "        without_reaction = delete_reaction_end(m, writers)\n",
    "        replaced_emojis = replace_hidden_emoji(without_reaction)\n",
    "        emojis = extract_emojis(replaced_emojis.split())\n",
    "        if emojis:\n",
    "            writers_emojis[names[i]] = writers_emojis.get(names[i], []) + [emojis]\n",
    "    return writers_emojis\n",
    "\n",
    "def get_used_emojis(writers, messages):    \n",
    "    written_emojis = get_writers_emojis(writers, messages)\n",
    "    used_emojis = {}\n",
    "    for writer in writers:\n",
    "        used_emojis[writer] = {}\n",
    "        if writer in written_emojis.keys():\n",
    "            for e in written_emojis[writer]:\n",
    "                for emoji_list in e:\n",
    "                    for emoji in emoji_list:\n",
    "                        used_emojis[writer][emoji] = used_emojis[writer].get(emoji, 0) + 1\n",
    "    return  used_emojis\n",
    "\n",
    "def get_reactions_to_messages(writers, names, messages):\n",
    "    reactions = {}\n",
    "    for writer in writers:\n",
    "        reactions[writer] = get_messages_which_was_reacted_to(writers, names, messages, writer)\n",
    "    return reactions\n",
    "\n",
    "def get_emoji_reaction_count(writers, messages):\n",
    "    emoji_reactions = {}\n",
    "    reactions = get_reactions_to_messages(writers, names, messages)\n",
    "    for writer in writers:\n",
    "        emoji_reactions[writer] = {}\n",
    "        for r in reactions[writer]:\n",
    "            reaction = r[1]\n",
    "            emoji_reactions[writer][reaction] = emoji_reactions[writer].get(reaction, 0) + 1\n",
    "    return emoji_reactions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_emoji_histogram(writers, messages, nonames = False):\n",
    "    used_emojis = get_used_emojis(writers, messages)\n",
    "\n",
    "    if nonames:\n",
    "        writers = [\"Author_\" + f\"{i}\" for i in range(len(writers))]\n",
    "        new_used_emoji = {}\n",
    "        for i, w in enumerate(list(used_emojis.keys())):\n",
    "            new_used_emoji[writers[i]] = used_emojis[w]\n",
    "        used_emojis = new_used_emoji\n",
    "\n",
    "    fig = go.FigureWidget()\n",
    "    for i in range(len(writers)):\n",
    "        fig.add_trace(go.Histogram(histfunc=\"sum\", y=list(used_emojis[writers[i]].values()), x=list(used_emojis[writers[i]].keys()), name=writers[i]))\n",
    "\n",
    "    fig.update_layout(title=\"Jak√© emoji pou≈æ√≠v√°m?\", xaxis_title='Emoji', yaxis_title='Poƒçet', width=1200)\n",
    "    # fig.show()\n",
    "    return fig\n",
    "\n",
    "# display_emoji_histogram(writers, messages_without_reactions, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_emoji_reactions(writers, messages, nonames=False):\n",
    "    emoji_reactions = get_emoji_reaction_count(writers, messages)\n",
    "\n",
    "    if nonames:\n",
    "        writers = [\"Author_\" + f\"{i}\" for i in range(len(writers))]\n",
    "        new_emoji_react = {}\n",
    "        for i, w in enumerate(list(emoji_reactions.keys())):\n",
    "            new_emoji_react[writers[i]] = emoji_reactions[w]\n",
    "        emoji_reactions = new_emoji_react\n",
    "            \n",
    "    fig = go.FigureWidget()\n",
    "    for i in range(len(writers)):\n",
    "        fig.add_trace(go.Histogram(histfunc=\"sum\", y=list(emoji_reactions[writers[i]].values()), x=list(emoji_reactions[writers[i]].keys()), name=writers[i]))\n",
    "\n",
    "    fig.update_layout(title=\"Jak√© reakce jsem dostal?\", xaxis_title='Emoji', yaxis_title='Poƒçet', width=1000)\n",
    "    # fig.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "# display_emoji_reactions(writers, messages, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}